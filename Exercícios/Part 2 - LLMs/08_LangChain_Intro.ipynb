{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Oitavo Trabalho**\n",
        "\n",
        "**Nome: Valéria Cristina A. R. de Figueredo**"
      ],
      "metadata": {
        "id": "o0Go6Tcilre1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_openai"
      ],
      "metadata": {
        "id": "oqTy6Bg_mBDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JQmPO5tjlk3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339c1b3c-d464-46d2-85ef-7dccd8fa6c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbs5Fqy-lk3n"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "sZHa9rq8pMVa"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "pjlHYgUElk3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfc6286-5317-4627-915c-da46afbc6297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá! Como posso ajudar você hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-1f51d9c8-13b0-42b6-a212-d320e04aef62-0' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Olá\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XWSJHillk3p"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQh--vRelk3p"
      },
      "source": [
        "### Templates Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "wvB1mfcPlk3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df321a4-cf5d-47d8-8914-ab92416ef8df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para português: Artificial Intelligence is the future!', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para português: {texto}\")\n",
        "\n",
        "prompt = prompt_template.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "WZG93Tbwlk3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d25bbf9-d915-465e-d31e-211850a76d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Inteligência Artificial é o futuro!\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMDyh2HYlk3q"
      },
      "source": [
        "### Templates de Mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "w1szScN9lk3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d8cddce-d9bf-43c5-bff8-e87b5ebcd9af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá, como você está?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 36, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-917509ae-0a54-47f4-b32e-dded83c439e0-0' usage_metadata={'input_tokens': 36, 'output_tokens': 7, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "0Q4scTwblk3r"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Z0__AsVjlk3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa52731-214f-44f9-af02-4ba99626c6c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"português\",\n",
        "    \"texto\": \"Hello, how are you?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Pf-X2jtOlk3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3f4d30-5162-46b2-b79d-6c77fe1217fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá, como você está?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6fz0zZ3lk3r"
      },
      "source": [
        "### Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Q6JBtFYnlk3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c2b6fb-bd0e-4dc9-ce33-ba90a476f171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='A capital do Rio Grande do Norte é Natal.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 16, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-4a9a9395-bf99-4e86-bc2b-5b7499e8aec5-0' usage_metadata={'input_tokens': 16, 'output_tokens': 11, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Saída do parser:\n",
            "A capital do Rio Grande do Norte é Natal.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "str_parser = StrOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Qual a capital do Rio Grande do Norte?\")\n",
        "output = str_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Saída do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "LtCXNEIQlk3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1c20a9-3750-4ea3-c394-37445e782643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='Claro! Aqui está a representação das massas e cargas das partículas que constituem o átomo em formato JSON:\\n\\n```json\\n{\\n  \"próton\": {\\n    \"massa\": \"1.6726 x 10^-27 kg\",\\n    \"carga\": \"+1 e\"\\n  },\\n  \"nêutron\": {\\n    \"massa\": \"1.6750 x 10^-27 kg\",\\n    \"carga\": \"0 e\"\\n  },\\n  \"elétron\": {\\n    \"massa\": \"9.1094 x 10^-31 kg\",\\n    \"carga\": \"-1 e\"\\n  }\\n}\\n```\\n\\nNeste formato, a massa está expressa em quilogramas e a carga em unidades de carga elementar (e).' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 38, 'total_tokens': 194, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-b9d75213-c18d-4b52-ab52-75ed5385d54d-0' usage_metadata={'input_tokens': 38, 'output_tokens': 156, 'total_tokens': 194, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Saída do parser:\n",
            "{'próton': {'massa': '1.6726 x 10^-27 kg', 'carga': '+1 e'}, 'nêutron': {'massa': '1.6750 x 10^-27 kg', 'carga': '0 e'}, 'elétron': {'massa': '9.1094 x 10^-31 kg', 'carga': '-1 e'}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Quais as massas e cargas das partículas que constituem o átomo? Responda no formato JSON em que cada chave seja o nome da partícula\")\n",
        "output = json_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Saída do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H09R3-Slk3s"
      },
      "source": [
        "## Encadeamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "grsIdydZlk3s"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ZeYGvWR-lk3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29929d8d-a1b8-4938-be4c-9ac0b6d96f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubarões!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "G3hMaYvYlk3s"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-jZhJ0iilk3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e9104e-f214-4399-af46-c07ffad9652b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"inglês\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obtuY__1lk3t"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxip5wR9lk3t"
      },
      "source": [
        "### Exercício 1\n",
        "Crie uma `chain` que a partir de um tópico informado pelo usuário, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "uYyb4r84lk3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ae2be2-5cc8-45a7-9293-b835a63a688e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Piada sobre advogado :\n",
            "Por que o advogado nunca joga esconde-esconde?\n",
            "\n",
            "Porque ele sempre acaba encontrando uma causa!\n"
          ]
        }
      ],
      "source": [
        "# Exercício 1\n",
        "\n",
        "\n",
        "\n",
        "# Define um prompt template para gerar uma piada baseada em um tópico\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um comediante. Crie uma piada engraçada sobre o seguinte tópico: {topico}\"),\n",
        "        (\"user\", \"{topico}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Função para criar uma piada de um tópico\n",
        "def create_joke(topico):\n",
        "    # Gerar o prompt\n",
        "    prompt = prompt_template.invoke({\"topico\": topico})\n",
        "\n",
        "    # Pegar a piada- resposta de um modelo de linguagem\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "# Testar a função com um tópico providenciado pelos usuários\n",
        "topic = \"advogado\"\n",
        "joke = create_joke(topic)\n",
        "\n",
        "print(\"Piada sobre\", topic, \":\")\n",
        "print(joke)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMU84v5Ylk3t"
      },
      "source": [
        "### Exercício 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "vo_ZXPPClk3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2daaa67-e560-4d58-8157-8feae8b4f4de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentimento do texto: 'Eu adoro este lugar, é incrível!' é: O sentimento do texto é 'positivo'.\n"
          ]
        }
      ],
      "source": [
        "# Exercício 2\n",
        "\n",
        "\n",
        "# Definir o template de prompt para a classificação de sentimento\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um analisador de sentimentos. Classifique o sentimento do seguinte texto como 'positivo', 'neutro' ou 'negativo':\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Função para classificar o sentimento de um texto\n",
        "def classify_sentiment(texto):\n",
        "    # Gera o prompt com o texto\n",
        "    prompt = prompt_template.invoke({\"texto\": texto})\n",
        "\n",
        "    # Obtem a resposta do modelo sobre o sentimento\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    # Usa o StrOutputParser para extrair a classificação\n",
        "    parser = StrOutputParser()\n",
        "    sentiment = parser.invoke(response)\n",
        "\n",
        "    return sentiment.strip()  # Remove espaços extras na resposta\n",
        "\n",
        "# Teste da função com um texto\n",
        "texto = \"Eu adoro este lugar, é incrível!\"\n",
        "sentimento = classify_sentiment(texto)\n",
        "\n",
        "print(f\"Sentimento do texto: '{texto}' é: {sentimento}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfSJuEsclk3t"
      },
      "source": [
        "### Exercício 3\n",
        "Crie uma `chain` que gere o código de uma função Python de acordo com a descrição do usuário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Aciur8RJlk3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f7fe28-8bce-4d10-8cbe-28a35b9b0f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Código gerado para a descrição 'Crie uma função que calcule o fatorial de um número':\n",
            "Claro! Aqui está uma função em Python que calcula o fatorial de um número:\n",
            "\n",
            "```python\n",
            "def fatorial(n):\n",
            "    \"\"\"\n",
            "    Calcula o fatorial de um número inteiro não negativo.\n",
            "\n",
            "    :param n: Número inteiro não negativo\n",
            "    :return: Fatorial de n\n",
            "    \"\"\"\n",
            "    if n < 0:\n",
            "        raise ValueError(\"O fatorial não está definido para números negativos.\")\n",
            "    elif n == 0 or n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        resultado = 1\n",
            "        for i in range(2, n + 1):\n",
            "            resultado *= i\n",
            "        return resultado\n",
            "\n",
            "# Exemplo de uso\n",
            "numero = 5\n",
            "print(f\"O fatorial de {numero} é {fatorial(numero)}\")\n",
            "```\n",
            "\n",
            "Essa função `fatorial` aceita um número inteiro não negativo e retorna o seu fatorial. Se o número for negativo, a função levanta uma exceção. O exemplo ao final mostra como usar a função.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Definição do template de prompt para a geração de código Python\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um programador Python. Gere o código de uma função Python de acordo com a seguinte descrição:\"),\n",
        "        (\"user\", \"{descricao}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Função para gerar código Python a partir de uma descrição\n",
        "def generate_python_function(descricao):\n",
        "    # Gera o prompt com a descrição\n",
        "    prompt = prompt_template.invoke({\"descricao\": descricao})\n",
        "\n",
        "    # Obtém a resposta do modelo com o código gerado\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    # Usa o StrOutputParser para formatar o código\n",
        "    parser = StrOutputParser()\n",
        "    code = parser.invoke(response)\n",
        "\n",
        "    return code.strip()  # Remove espaços extras na resposta\n",
        "\n",
        "# Teste da função com uma descrição de função\n",
        "descricao = \"Crie uma função que calcule o fatorial de um número\"\n",
        "codigo_gerado = generate_python_function(descricao)\n",
        "\n",
        "print(f\"Código gerado para a descrição '{descricao}':\")\n",
        "print(codigo_gerado)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMVssEBxlk3t"
      },
      "source": [
        "### Exercício 4\n",
        "Crie uma `chain` que explique de forma simplificada um tópico geral fornecido pelo usuário e, em seguida, traduza a explicação para inglês. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "H0s1wdpglk3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813571f4-9a32-4762-dcec-dff1f416441d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explicação em português: \"Romeu e Julieta\" é uma famosa tragédia escrita por William Shakespeare. A história se passa em Verona, Itália, e fala sobre dois jovens apaixonados, Romeu e Julieta, que pertencem a famílias rivais: os Montéquios e os Capuletos.\n",
            "\n",
            "No início da peça, Romeu está triste porque está apaixonado por outra garota, mas, ao ir a uma festa dos Capuletos, ele conhece Julieta e logo se apaixonam. Apesar de suas famílias serem inimigas, eles decidem se casar em segredo.\n",
            "\n",
            "No entanto, uma série de mal-entendidos e tragédias acontece. O primo de Julieta, Teobaldo, desafia Romeu para um duelo. Romeu tenta evitar a briga, mas acaba matando Teobaldo em um momento de raiva. Como punição, ele é banido de Verona.\n",
            "\n",
            "Julieta, desesperada, procura uma solução e um plano arriscado é criado: ela toma uma poção que a faz parecer morta por 42 horas. A ideia é que, quando acordar, Romeu a encontrará e eles poderão fugir juntos. No entanto, o plano não dá certo. Romeu, ao ouvir que Julieta está morta, fica devastado e toma veneno para se juntar a ela na morte. Quando Julieta acorda e vê que Romeu morreu, ela também se mata.\n",
            "\n",
            "A história de Romeu e Julieta é uma tragédia sobre amor, conflito e as consequências de rivalidades familiares. Ela nos ensina que o amor verdadeiro pode enfrentar obstáculos, mas, em alguns casos, pode ter um preço muito alto.\n",
            "\n",
            "Explicação traduzida para inglês: \"Romeo and Juliet\" is a famous tragedy written by William Shakespeare. The story takes place in Verona, Italy, and tells the tale of two young lovers, Romeo and Juliet, who belong to rival families: the Montagues and the Capulets.\n",
            "\n",
            "At the beginning of the play, Romeo is sad because he is in love with another girl, but when he goes to a Capulet party, he meets Juliet and they quickly fall in love. Despite their families being enemies, they decide to marry in secret.\n",
            "\n",
            "However, a series of misunderstandings and tragedies occur. Juliet's cousin, Tybalt, challenges Romeo to a duel. Romeo tries to avoid the fight but ends up killing Tybalt in a moment of rage. As punishment, he is banished from Verona.\n",
            "\n",
            "Juliet, desperate, seeks a solution, and a risky plan is devised: she takes a potion that makes her appear dead for 42 hours. The idea is that when she wakes up, Romeo will find her and they can escape together. However, the plan goes awry. Romeo, upon hearing that Juliet is dead, is devastated and takes poison to join her in death. When Juliet wakes up and sees that Romeo is dead, she also takes her own life.\n",
            "\n",
            "The story of Romeo and Juliet is a tragedy about love, conflict, and the consequences of family rivalries. It teaches us that true love can face obstacles, but in some cases, it can come at a very high price.\n"
          ]
        }
      ],
      "source": [
        "# Define o template para explicar o tópico de forma simplificada\n",
        "explanation_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um professor. Explique de forma simples o seguinte tópico:\"),\n",
        "        (\"user\", \"{topico}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define o template para traduzir a explicação para o inglês\n",
        "translation_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um tradutor. Traduza o seguinte texto para o inglês:\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Função para gerar a explicação simplificada do tópico\n",
        "def explain_topic(topico):\n",
        "    # Gera o prompt para a explicação\n",
        "    explanation_prompt = explanation_template.invoke({\"topico\": topico})\n",
        "\n",
        "    # Obtém a explicação do modelo\n",
        "    explanation = llm.invoke(explanation_prompt)\n",
        "\n",
        "    return explanation.content.strip()\n",
        "\n",
        "# Função para traduzir a explicação para o inglês\n",
        "def translate_to_english(texto):\n",
        "    # Gera o prompt para tradução\n",
        "    translation_prompt = translation_template.invoke({\"texto\": texto})\n",
        "\n",
        "    # Obtém a tradução do modelo\n",
        "    translation = llm.invoke(translation_prompt)\n",
        "\n",
        "    return translation.content.strip()\n",
        "\n",
        "# Função completa que explica o tópico e traduz a explicação\n",
        "def explain_and_translate(topico):\n",
        "    # Passo 1: Explicar o tópico de forma simples\n",
        "    explanation = explain_topic(topico)\n",
        "    print(f\"Explicação em português: {explanation}\\n\")\n",
        "\n",
        "    # Passo 2: Traduzir a explicação para o inglês\n",
        "    translation = translate_to_english(explanation)\n",
        "    return translation\n",
        "\n",
        "# Teste do tópico fornecido pelo usuário\n",
        "topico = \"A história de Romeu e Julieta\"\n",
        "translated_explanation = explain_and_translate(topico)\n",
        "\n",
        "print(f\"Explicação traduzida para inglês: {translated_explanation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JPEieiplk3u"
      },
      "source": [
        "### Exercício 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "6Q7gOFFXlk3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69df14d0-93e8-4420-d6d4-bc153c9f230a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta para a pergunta 'Quais cursos são oferecidos pelo CESAR School?': A CESAR School oferece uma variedade de cursos nas áreas de tecnologia, design e inovação. Entre os principais cursos, estão:\n",
            "\n",
            "1. **Graduação**:\n",
            "   - Análise e Desenvolvimento de Sistemas\n",
            "   - Design de Interação\n",
            "   - Engenharia de Software\n",
            "\n",
            "2. **Pós-graduação**:\n",
            "   - MBA em Gestão de Produtos\n",
            "   - MBA em Gestão da Inovação\n",
            "   - Especialização em Design de Experiência do Usuário (UX)\n",
            "   - Especialização em Inteligência Artificial e Data Science\n",
            "\n",
            "3. **Cursos de Extensão e Formação**:\n",
            "   - Cursos voltados para desenvolvimento de habilidades específicas, como programação, design, metodologias ágeis, entre outros.\n",
            "\n",
            "A CESAR School é conhecida por sua abordagem prática e focada em projetos, além de contar com um corpo docente formado por profissionais atuantes no mercado. É importante verificar o site oficial da CESAR School para obter informações atualizadas sobre os cursos e suas respectivas inscrições.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Definição do template de prompt para responder perguntas sobre o CESAR School\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um especialista em CESAR School. Responda a perguntas sobre a escola com base no que você sabe.\"),\n",
        "        (\"user\", \"{pergunta}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Função para responder perguntas sobre o CESAR School\n",
        "def answer_cesar_question(pergunta):\n",
        "    # Gerar o prompt com a pergunta do usuário\n",
        "    prompt = prompt_template.invoke({\"pergunta\": pergunta})\n",
        "\n",
        "    # Obtém a resposta do modelo\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    # Utilização de um parser para formatar a resposta, caso necessário\n",
        "    parser = StrOutputParser()\n",
        "    answer = parser.invoke(response)\n",
        "\n",
        "    return answer.strip()  # Remove espaços extras na resposta\n",
        "\n",
        "# Teste da função com uma pergunta sobre o CESAR School\n",
        "pergunta = \"Quais cursos são oferecidos pelo CESAR School?\"\n",
        "resposta = answer_cesar_question(pergunta)\n",
        "\n",
        "print(f\"Resposta para a pergunta '{pergunta}': {resposta}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}